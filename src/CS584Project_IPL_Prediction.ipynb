{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb0b9e0fcfecd8bd2e0b2323d34bed825f54f01a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "import pandas\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4db67b9650ad6a5d01bd7dccb11d922b891b1b9b"
   },
   "outputs": [],
   "source": [
    "iplmatches = pd.read_csv('data/Matches.csv')\n",
    "ipldelivery = pd.read_csv('data/Deliveries.csv')\n",
    "\n",
    "                                  \n",
    "Tiplmatches = pd.read_csv('data/Matches IPL 2020.csv')\n",
    "Tipldelivery = pd.read_csv('data/Deliveries IPL 2020.csv')\n",
    "Tiplmatches = Tiplmatches.replace('Rising Pune Supergiant', 'Rising Pune Supergiants')\n",
    "Tiplmatches = Tiplmatches.replace('Delhi Capitals', 'Delhi Daredevils')\n",
    "Tipldelivery = Tipldelivery.replace('Rising Pune Supergiant', 'Rising Pune Supergiants')\n",
    "Tipldelivery = Tipldelivery.replace('Delhi Capitals', 'Delhi Daredevils')\n",
    "\n",
    "\n",
    "iplmatches = pd.concat([iplmatches, Tiplmatches], ignore_index = True)\n",
    "ipldelivery = pd.concat([ipldelivery, Tipldelivery], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplmatches = iplmatches.replace('Rising Pune Supergiant', 'Rising Pune Supergiants')\n",
    "iplmatches = iplmatches.replace('Delhi Capitals', 'Delhi Daredevils')\n",
    "iplmatches = iplmatches.replace('CSK', 'Chennai Super Kings')\n",
    "iplmatches = iplmatches.replace('KXIP', 'Kings XI Punjab')\n",
    "iplmatches = iplmatches.replace('SRH', 'Sunrisers Hyderabad')\n",
    "iplmatches = iplmatches.replace('KKR', 'Kolkata Knight Riders')\n",
    "iplmatches = iplmatches.replace('RCB', 'Royal Challengers Bangalore')\n",
    "iplmatches = iplmatches.replace('RR', 'Rajasthan Royals')\n",
    "iplmatches = iplmatches.replace('MI', 'Mumbai Indians')\n",
    "iplmatches = iplmatches.replace('DC', 'Delhi Daredevils')\n",
    "\n",
    "\n",
    "ipldelivery = ipldelivery.replace('Rising Pune Supergiant', 'Rising Pune Supergiants')\n",
    "ipldelivery = ipldelivery.replace('Delhi Capitals', 'Delhi Daredevils')\n",
    "ipldelivery = ipldelivery.replace('CSK', 'Chennai Super Kings')\n",
    "ipldelivery = ipldelivery.replace('KXIP', 'Kings XI Punjab')\n",
    "ipldelivery = ipldelivery.replace('SRH', 'Sunrisers Hyderabad')\n",
    "ipldelivery = ipldelivery.replace('KKR', 'Kolkata Knight Riders')\n",
    "ipldelivery = ipldelivery.replace('RCB', 'Royal Challengers Bangalore')\n",
    "ipldelivery = ipldelivery.replace('RR', 'Rajasthan Royals')\n",
    "ipldelivery = ipldelivery.replace('MI', 'Mumbai Indians')\n",
    "ipldelivery = ipldelivery.replace('DC', 'Delhi Daredevils')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a985168ca61b451fc95192010f5cad113f25d0ed"
   },
   "outputs": [],
   "source": [
    "ipldelivery.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b14bc55d1fac5fb18a2f693478af513a16e73064"
   },
   "outputs": [],
   "source": [
    "\n",
    "batsman_grp = ipldelivery.groupby([\"match_id\", \"inning\", \"batting_team\", \"batsman\"])\n",
    "batsman_grp.head(5)\n",
    "batsmen = batsman_grp[\"batsman_runs\"].sum().reset_index()\n",
    "batsmen\n",
    "batsmen = iplmatches[['id','season']].merge(batsmen, left_on = 'id', right_on = 'match_id', how = 'left').drop('id', axis = 1)\n",
    "batsmen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2238b67daf20e8ae453503568105f19785d002fe"
   },
   "outputs": [],
   "source": [
    "bowler_grp = ipldelivery.groupby([\"match_id\", \"inning\", \"bowling_team\", \"bowler\", \"over\"])\n",
    "bowlers = bowler_grp[\"total_runs\"].sum().reset_index()\n",
    "\n",
    "\n",
    "dismissal_kinds_for_bowler = [\"bowled\", \"caught\", \"lbw\", \"stumped\", \"caught and bowled\", \"hit wicket\"]\n",
    "dismissals = ipldelivery[ipldelivery[\"dismissal_kind\"].isin(dismissal_kinds_for_bowler)]\n",
    "dismissals = dismissals.groupby([\"match_id\", \"inning\", \"bowling_team\", \"bowler\", \"over\"])[\"dismissal_kind\"].count().reset_index()\n",
    "dismissals.rename(columns={\"dismissal_kind\": \"wickets\"}, inplace=True)\n",
    "\n",
    "bowlers = bowlers.merge(dismissals, left_on=[\"match_id\", \"inning\", \"bowling_team\", \"bowler\", \"over\"], \n",
    "                        right_on=[\"match_id\", \"inning\", \"bowling_team\", \"bowler\", \"over\"], how=\"left\")\n",
    "bowlers[\"wickets\"] = bowlers[\"wickets\"].fillna(0)\n",
    "\n",
    "bowlers_over = bowlers.groupby(['match_id', 'inning', 'bowling_team', 'bowler'])['over'].count().reset_index()\n",
    "bowlers = bowlers.groupby(['match_id', 'inning', 'bowling_team', 'bowler']).sum().reset_index().drop('over', 1)\n",
    "bowlers = bowlers_over.merge(bowlers, on=[\"match_id\", \"inning\", \"bowling_team\", \"bowler\"], how = 'left')\n",
    "# bowlers['Econ'] = np.round(bowlers['runs'] / bowlers['over'] , 2)\n",
    "\n",
    "bowlers = iplmatches[['id','season']].merge(bowlers, left_on = 'id', right_on = 'match_id', how = 'left').drop('id', axis = 1)\n",
    "\n",
    "bowlers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fb1030282681c758cac5dea7ebd4a57b80d3d95"
   },
   "outputs": [],
   "source": [
    "iplmatches.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ebee6f2438a7a26ed40b1f3b0d522984d7e8676"
   },
   "outputs": [],
   "source": [
    "winneroftoss = iplmatches[(iplmatches['toss_winner']) == (iplmatches['winner'])]\n",
    "\n",
    "wot = sns.countplot( x = 'winner', hue='season', data=winneroftoss)\n",
    "sns.set(rc={'figure.figsize':(8,6)})\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel(\"Teams\")\n",
    "plt.ylabel(\"Number of Wins\")\n",
    "plt.title(\"Number of Teams who won, given they win the toss, every season\")\n",
    "plt.show(wot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b2bdbcbe5777de74e6806b5de40cdd93f0b5bb8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "import pandas\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplmatches.toss_winner.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31c47bbd9a8282d6e56982906291fa026883e1ff"
   },
   "outputs": [],
   "source": [
    "matches = iplmatches.copy()\n",
    "matches = matches.drop(columns = ['id', 'date', 'result', 'dl_applied', 'player_of_match', 'umpire2', 'umpire3'])\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae075836201b80e65b7e9b8e74ec695c8415311d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matches.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "852b66e8b336cfcd4cc34751d259f92157c90a45"
   },
   "outputs": [],
   "source": [
    "copy_data = matches.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80857b88f5494babbc89f6780367d4e2399f3c0f"
   },
   "outputs": [],
   "source": [
    "copy_data['city'].fillna('Dubai',inplace=True)\n",
    "copy_data['umpire1'].fillna('Aleem Dar',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9c441fb868968dec8f24f2bb50557c892f3b5d6"
   },
   "outputs": [],
   "source": [
    "null_values_col = copy_data.isnull().sum()\n",
    "null_values_col = null_values_col[null_values_col != 0].sort_values(ascending = False).reset_index()\n",
    "null_values_col.columns = [\"variable\", \"number of missing\"]\n",
    "null_values_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b76267cfd83be308627ad55997dc54f37f235d02",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(copy_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpy_batsmen = batsmen\n",
    "cpy_batsmen =cpy_batsmen.drop(columns = ['batsman'])\n",
    "cpy_batsmen = cpy_batsmen.groupby([\"season\", \"match_id\", \"inning\", \"batting_team\"])\n",
    "cpy_batsmen.head()\n",
    "\n",
    "team_runs = cpy_batsmen[\"batsman_runs\"].sum().reset_index()\n",
    "team_runs=team_runs.rename(columns={'batsman_runs': \"avgRuns\"})\n",
    "runs_year = team_runs.groupby(['season', 'batting_team']).mean()\n",
    "runs_year= runs_year.drop(columns= ['match_id', 'inning'])\n",
    "dict1 = dict()\n",
    "for i,j in team_runs.groupby(['season', 'batting_team']):\n",
    "    print(i)\n",
    "    print(type(i))\n",
    "    print(i[0])\n",
    "    dict1[i] = j['avgRuns'].mean()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0d98b5ad2b7689ed62be4431cf93054f5f59fc16"
   },
   "outputs": [],
   "source": [
    "#Create now a dataframe copy of the data and all its rows and named columns.\n",
    "df = DataFrame(copy_data,columns=['team1', 'team2', 'toss_decision','toss_winner', 'city', 'venue', 'season', 'win_by_runs', 'win_by_wickets', 'umpire1', 'winner'])\n",
    "df['avgTeam1'] = 0\n",
    "df['avgTeam2'] = 0\n",
    "# Set avg team1 to previous season avg\n",
    "for i , j in dict1:\n",
    "    filter1 = team_runs['season'] == i\n",
    "    filter2 = team_runs['batting_team'] == j\n",
    "    a = team_runs.where(filter1&filter2)\n",
    "    a=a.dropna().reset_index()\n",
    "    a= a.drop(columns = 'index') \n",
    "    tst = (i-1 , j)\n",
    "    if i == 2008 :\n",
    "        a['avgRuns'] = 0\n",
    "    else:    \n",
    "        if(tst in dict1):\n",
    "            a['avgRuns'] = dict1[i-1,j]\n",
    "#         else:\n",
    "#             a['avgRuns'] = 0        \n",
    "\n",
    "    \n",
    "    for k, ro in a.iterrows():\n",
    "        for l, row in df.iterrows():\n",
    "            if(ro[0] == row[6] and ro[3] == row[0]):\n",
    "                df.at[l , 'avgTeam1']=ro[4] \n",
    "        break\n",
    "\n",
    "# Set avg team2 to previous season avg        \n",
    "for i , j in dict1:\n",
    "    filter1 = team_runs['season'] == i\n",
    "    filter2 = team_runs['batting_team'] == j\n",
    "    a = team_runs.where(filter1&filter2)\n",
    "    a=a.dropna().reset_index()\n",
    "    a= a.drop(columns = 'index') \n",
    "#     if j in('Kochi Tuskers Kerala','Pune Warriors','Sunrisers Hyderabad','Gujarat Lions','Rising Pune Supergiants','Rising Pune Supergiant'):\n",
    "#         continue\n",
    "    tst = (i-1 , j)\n",
    "    if i == 2008 :\n",
    "#         print('i = 2008')\n",
    "        a['avgRuns'] = 0\n",
    "    else:    \n",
    "        if(tst in dict1):\n",
    "#             print( 'i is: ', i)\n",
    "            a['avgRuns'] = dict1[i-1,j]\n",
    "#         else:\n",
    "#             a['avgRuns'] = 0        \n",
    "#     print(a)\n",
    "\n",
    "    \n",
    "    for k, ro in a.iterrows():\n",
    "        for l, row in df.iterrows():\n",
    "            if(ro[0] == row[6] and ro[3] == row[1]):\n",
    "                df.at[l , 'avgTeam2']=ro[4] \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for avg wckts\n",
    "cpy_bowler = bowlers\n",
    "cpy_bowler =cpy_bowler.drop(columns = ['bowler','over', 'total_runs'])\n",
    "cpy_bowler = cpy_bowler.groupby([\"season\",\"match_id\", \"bowling_team\"])\n",
    "cpy_bowler.head()\n",
    "\n",
    "team_wickets = cpy_bowler[\"wickets\"].sum().reset_index()\n",
    "team_wickets=team_wickets.rename(columns={'wickets': \"avgWickets\"})\n",
    "team_wickets\n",
    "\n",
    "wickets_year = team_wickets.groupby(['season', 'bowling_team']).mean()\n",
    "wickets_year= wickets_year.drop(columns= ['match_id'])\n",
    "dict2 = dict()\n",
    "for i,j in team_wickets.groupby(['season', 'bowling_team']):\n",
    "#     print(i)\n",
    "#     print(type(i))\n",
    "#     print(i[0])\n",
    "    dict2[i] = j['avgWickets'].mean()\n",
    "\n",
    "df['avgWckTeam1'] = 0\n",
    "df['avgWckTeam2'] = 0\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set avgwckt for team 1 to previous season avg\n",
    "for i , j in dict2:\n",
    "    filter1 = team_wickets['season'] == i\n",
    "    filter2 = team_wickets['bowling_team'] == j\n",
    "    a = team_wickets.where(filter1&filter2)\n",
    "    a=a.dropna().reset_index()\n",
    "    a= a.drop(columns = 'index') \n",
    "    tst = (i-1 , j)\n",
    "    if i == 2008 :\n",
    "        a['avgWickets'] = 0\n",
    "    else:    \n",
    "        if(tst in dict1):\n",
    "            a['avgWickets'] = dict2[i-1,j]\n",
    "        else:\n",
    "            a['avgWickets'] = 0        \n",
    "\n",
    "\n",
    "    for k, ro in a.iterrows():\n",
    "        for l, row in df.iterrows():\n",
    "            if(ro[0] == row[6] and ro[2] == row[0]):\n",
    "                df.at[l , 'avgWckTeam1']=int(ro[3]) \n",
    "        break\n",
    "\n",
    "# set avgwckt for team 2 to previous season avg\n",
    "for i , j in dict2:\n",
    "    filter1 = team_wickets['season'] == i\n",
    "    filter2 = team_wickets['bowling_team'] == j\n",
    "    a = team_wickets.where(filter1&filter2)\n",
    "    a=a.dropna().reset_index()\n",
    "    a= a.drop(columns = 'index') \n",
    "    tst = (i-1 , j)\n",
    "    if i == 2008 :\n",
    "        a['avgWickets'] = 0\n",
    "    else:    \n",
    "        if(tst in dict1):\n",
    "            a['avgWickets'] = dict2[i-1,j]\n",
    "        else:\n",
    "            a['avgWickets'] = 0        \n",
    "\n",
    "\n",
    "    for k, ro in a.iterrows():\n",
    "        for l, row in df.iterrows():\n",
    "            if(ro[0] == row[6] and ro[2] == row[1]):\n",
    "                df.at[l , 'avgWckTeam2']=int(ro[3]) \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3cfa28147347e8ee2c8c12eb3983955765ab050"
   },
   "outputs": [],
   "source": [
    " df['winner'].fillna('Draw', inplace=True)\n",
    "df.replace(['Mumbai Indians','Kolkata Knight Riders','Royal Challengers Bangalore','Deccan Chargers','Chennai Super Kings',\n",
    "                 'Rajasthan Royals','Delhi Daredevils','Gujarat Lions','Kings XI Punjab',\n",
    "                 'Sunrisers Hyderabad','Rising Pune Supergiants','Kochi Tuskers Kerala','Pune Warriors']\n",
    "                ,['MI','KKR','RCB','DC','CSK','RR','DD','GL','KXIP','SRH','RPS','KTK','PW'],inplace=True)\n",
    "\n",
    "encode = {'team1': {'MI':1,'KKR':2,'RCB':3,'DC':4,'CSK':5,'RR':6,'DD':7,'GL':8,'KXIP':9,'SRH':10,'RPS':11,'KTK':12,'PW':13},\n",
    "          'team2': {'MI':1,'KKR':2,'RCB':3,'DC':4,'CSK':5,'RR':6,'DD':7,'GL':8,'KXIP':9,'SRH':10,'RPS':11,'KTK':12,'PW':13},\n",
    "          'toss_winner': {'MI':1,'KKR':2,'RCB':3,'DC':4,'CSK':5,'RR':6,'DD':7,'GL':8,'KXIP':9,'SRH':10,'RPS':11,'KTK':12,'PW':13},\n",
    "          'winner': {'MI':1,'KKR':2,'RCB':3,'DC':4,'CSK':5,'RR':6,'DD':7,'GL':8,'KXIP':9,'SRH':10,'RPS':11,'KTK':12,'PW':13,'Draw':14}}\n",
    "df.replace(encode, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "742f5d348933b28c17540959aa3fe9248b113664"
   },
   "outputs": [],
   "source": [
    "dicVal = encode['winner']\n",
    "print(dicVal['MI']) #key value\n",
    "print(list(dicVal.keys())[list(dicVal.values()).index(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94dded967bdc07048b5eb56b0862314a02d0691d"
   },
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c78a5c603c8e1c01c201e537e1f12de4efe25cc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.toss_decision = pd.Categorical(df.toss_decision)\n",
    "df['toss_decision'] = df.toss_decision.cat.codes\n",
    "df.city = pd.Categorical(df.city)\n",
    "df['city'] = df.city.cat.codes\n",
    "df.venue = pd.Categorical(df.venue)\n",
    "df['venue'] = df.venue.cat.codes\n",
    "df.umpire1 = pd.Categorical(df.umpire1)\n",
    "df['umpire1'] = df.umpire1.cat.codes\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4df20e32740b430c4a3cb3c2fcc6285c73222bc0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Compare the data at the beginning to now, ensuring no string value remains.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outcome variable team1_win as a probability of team1 winning the match\n",
    "df.loc[df[\"winner\"]==df[\"team1\"],\"team1_win\"]=1\n",
    "df.loc[df[\"winner\"]!=df[\"team1\"],\"team1_win\"]=0\n",
    "\n",
    "#outcome variable team1_toss_win as a value of team1 winning the toss\n",
    "df.loc[df[\"toss_winner\"]==df[\"team1\"],\"team1_toss_win\"]=1\n",
    "df.loc[df[\"toss_winner\"]!=df[\"team1\"],\"team1_toss_win\"]=0\n",
    "\n",
    "#outcome variable team1_bat to depict if team1 bats first\n",
    "df[\"team1_bat\"]=0\n",
    "df.loc[(df[\"team1_toss_win\"]==1) & (df[\"toss_decision\"]==0),\"team1_bat\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dataframe of related features\n",
    "prediction_df=df[[\"team1\",\"team2\",\"team1_toss_win\",\"team1_bat\",\"team1_win\",\"venue\"]]\n",
    "\n",
    "#finding the higly correlated features\n",
    "correlated_features = set()\n",
    "correlation_matrix = prediction_df.drop('team1_win', axis=1).corr()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            column = correlation_matrix.columns[i]\n",
    "            correlated_features.add(column)\n",
    "            \n",
    "prediction_df.drop(columns=correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features = ['city', 'venue', 'avgTeam1', 'avgTeam2', 'avgWckTeam1', 'avgWckTeam2', 'umpire1']\n",
    "\n",
    "df_norm = df[features]\n",
    "\n",
    "x_norm = df_norm.values\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "x_scaled = min_max_scaler.fit_transform(x_norm)\n",
    "x_df = pd.DataFrame(x_scaled)\n",
    "\n",
    "x_df.columns = features\n",
    "df[features] = x_df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df.columns.tolist()\n",
    "\n",
    "feature_cols.remove('team1_win')\n",
    "feature_cols.remove('winner')\n",
    "\n",
    "\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random forest for feature importance on a classification problem\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "# fit the model\n",
    "model.fit(df[feature_cols], df['team1_win'])\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %s, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([feature_cols[x] for x in range(len(importance))], importance)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to csv\n",
    "\n",
    "df.to_csv('after_norm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.copy()\n",
    "\n",
    "df_test['avgTeam1_runs_greater'] = df_test.apply(lambda row: 1.0 if row.avgTeam1 > row.avgTeam2 else 0, axis = 1)\n",
    "df_test['avgTeam1_wickets_greater'] = df_test.apply(lambda row: 1.0 if row.avgWckTeam1 > row.avgWckTeam2 else 0, axis = 1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to csv\n",
    "\n",
    "df_test.to_csv('for_neural_net.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to find K?\n",
    "# Now we are going to split the training and test models in a typical 60:20:20 set.\n",
    "# x = df[['toss_decision', 'avgTeam1', 'avgTeam2', 'avgWckTeam1', 'avgWckTeam2', 'umpire1', 'team1_toss_win', 'team1_bat']]\n",
    "x = df[['toss_decision', 'avgTeam1', 'avgTeam2', 'avgWckTeam1', 'avgWckTeam2', 'team1_toss_win', 'team1_bat']]\n",
    "\n",
    "\n",
    "# x = df_test[['team1', 'team2', 'toss_decision', 'toss_winner' ,'city', 'season', 'umpire1', 'avgTeam1_runs_greater', 'avgTeam1_wickets_greater', 'team1_toss_win', 'team1_bat']]\n",
    "y = df_test[['team1_win']]\n",
    "\n",
    "# x_model, x_test, y_model, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_model, y_model, test_size=0.2, random_state=1)\n",
    "x_train = x[:756]\n",
    "y_train = y[:756]\n",
    "\n",
    "x_test = x[756:]\n",
    "y_test = y[756:]\n",
    "\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.to_csv(\"x_train.csv\")\n",
    "\n",
    "y_train.to_csv(\"y_train.csv\")\n",
    "\n",
    "x_test.to_csv(\"x_test.csv\")\n",
    "\n",
    "y_test.to_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPSAMPLING OF TRAIN DATA (from 2008-2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "a = Counter(y_train['team1_win'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([x_train, y_train], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_majority = train_data[train_data['team1_win']==0]\n",
    "train_data_minority = train_data[train_data['team1_win']==1]\n",
    "\n",
    "train_data_minority_upsampled = resample(train_data_minority, \n",
    "                                 replace=True,                  # sample without replacement\n",
    "                                 n_samples=len(train_data_majority),     # to match minority class\n",
    "                                 )               # reproducible results\n",
    "\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "train_data_upsampled = pd.concat([train_data_minority_upsampled, train_data_majority])\n",
    " \n",
    "# Display new class counts\n",
    "train_data_upsampled['team1_win'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data_upsampled[['toss_decision', 'avgTeam1', 'avgTeam2', 'avgWckTeam1', 'avgWckTeam2', 'team1_toss_win', 'team1_bat']]\n",
    "\n",
    "\n",
    "# x = df_test[['team1', 'team2', 'toss_decision', 'toss_winner' ,'city', 'season', 'umpire1', 'avgTeam1_runs_greater', 'avgTeam1_wickets_greater', 'team1_toss_win', 'team1_bat']]\n",
    "y_train = train_data_upsampled[['team1_win']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "train_scores = []\n",
    "validation_scores = []\n",
    "\n",
    "x_model_values = x_train.values\n",
    "y_model_values = y_train.values\n",
    "\n",
    "# 5-fold cross validation\n",
    "\n",
    "kfold = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "for i in range(1,100):\n",
    "    knn = KNeighborsClassifier(i)\n",
    "    \n",
    "    tr_scores = []\n",
    "    va_scores = []\n",
    "    \n",
    "    for a, b in kfold.split(x_model_values):\n",
    "\n",
    "        x_train_fold, y_train_fold = x_model_values[a], y_model_values[a]\n",
    "        x_val_fold, y_val_fold = x_model_values[b], y_model_values[b]\n",
    "        \n",
    "        knn.fit(x_train_fold, y_train_fold.ravel())\n",
    "        \n",
    "        va_scores.append(knn.score(x_val_fold, y_val_fold))\n",
    "        tr_scores.append(knn.score(x_train_fold, y_train_fold))\n",
    "        \n",
    "    validation_scores.append(np.mean(va_scores))\n",
    "    train_scores.append(np.mean(tr_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('k-NN - Accuracy vs k')\n",
    "plt.plot(range(1,100),validation_scores,label=\"Validation\")\n",
    "plt.plot(range(1,100),train_scores,label=\"Train\")\n",
    "plt.legend()\n",
    "plt.xticks(range(0,110, 10), rotation=90)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_params = {\n",
    "    'n_neighbors': [i for i in range(10,50,2)],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose=1, cv=10, n_jobs=-1)\n",
    "\n",
    "gs_results = gs.fit(x_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "results = cross_val_score(gs_results.best_estimator_, x_train, y_train.values.ravel(), cv=10)\n",
    "\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, shuffle=True)\n",
    "model=KNeighborsClassifier(metric='euclidean', n_neighbors=10, weights='distance') \n",
    "\n",
    "results = model_selection.cross_validate(estimator=model,\n",
    "                                          X=x_train,\n",
    "                                          y=y_train.values.ravel(),\n",
    "                                          cv=kfold,\n",
    "                                          scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "print(\"F1-score = \",mean(results['test_f1_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_results.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_params = {'C': [0.1, 0.5, 1, 10, 100],\n",
    "              'kernel': ['rbf', 'linear']}\n",
    "\n",
    "gs = GridSearchCV(SVC(gamma='auto'), grid_params, verbose=1, cv=10, n_jobs=-1)\n",
    "\n",
    "gs.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "results = cross_val_score(gs.best_estimator_, x_train, y_train.values.ravel(), cv=10)\n",
    "\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from statistics import mean\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, shuffle=True)\n",
    "model=gs.best_estimator_ \n",
    "\n",
    "results = model_selection.cross_validate(estimator=model,\n",
    "                                          X=x_train,\n",
    "                                          y=y_train.values.ravel(),\n",
    "                                          cv=kfold,\n",
    "                                          scoring=scoring)\n",
    "\n",
    "print(\"F1-score = \",mean(results['test_f1_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "grid_params = {'var_smoothing':[1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3]}\n",
    "\n",
    "gs = GridSearchCV(GaussianNB(), grid_params, verbose=1, cv=10, n_jobs=-1)\n",
    "\n",
    "gs_results_nb = gs.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "print(gs_results_nb.best_score_)\n",
    "print(gs_results_nb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results_nb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "results = cross_val_score(gs_results_nb.best_estimator_, x_train, y_train.values.ravel(), cv=10)\n",
    "\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from statistics import mean\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, shuffle=True)\n",
    "model=gs_results_nb.best_estimator_ \n",
    "\n",
    "results = model_selection.cross_validate(estimator=model,\n",
    "                                          X=x_train,\n",
    "                                          y=y_train.values.ravel(),\n",
    "                                          cv=kfold,\n",
    "                                          scoring=scoring)\n",
    "\n",
    "print(\"F1-score = \",mean(results['test_f1_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {'criterion': ['gini', 'entropy'],\n",
    "'max_depth': [i for i in range(2,40,3)],\n",
    "'min_samples_leaf': [i for i in range(2, 40, 3)]}\n",
    "\n",
    "gs = GridSearchCV(DecisionTreeClassifier(), grid_params, verbose=1, cv=10, n_jobs=-1)\n",
    "\n",
    "gs_results_dt = gs.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "print(gs_results_dt.best_score_)\n",
    "print(gs_results_dt.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_results_dt.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "results = cross_val_score(gs_results_dt.best_estimator_, x_train, y_train.values.ravel(), cv=10)\n",
    "\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from statistics import mean\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, shuffle=True)\n",
    "model=gs_results_dt.best_estimator_ \n",
    "\n",
    "results = model_selection.cross_validate(estimator=model,\n",
    "                                          X=x_train,\n",
    "                                          y=y_train.values.ravel(),\n",
    "                                          cv=kfold,\n",
    "                                          scoring=scoring)\n",
    "\n",
    "print(\"F1-score = \",mean(results['test_f1_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "grid_params = {'penalty':['l1', 'l2'],\n",
    "              'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "gs = GridSearchCV(LogisticRegression(), grid_params, verbose=1, cv=10, n_jobs=-1)\n",
    "\n",
    "gs_results_lr = gs.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "print(gs_results_lr.best_score_)\n",
    "print(gs_results_lr.best_estimator_)\n",
    "print(gs_results_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results_lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs_results_lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from statistics import mean\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, shuffle=True)\n",
    "model=gs_results_lr.best_estimator_ \n",
    "\n",
    "results = model_selection.cross_validate(estimator=model,\n",
    "                                          X=x_train,\n",
    "                                          y=y_train.values.ravel(),\n",
    "                                          cv=kfold,\n",
    "                                          scoring=scoring)\n",
    "\n",
    "print(\"F1-score = \",mean(results['test_f1_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "results = cross_val_score(gs_results_lr.best_estimator_, x_train, y_train.values.ravel(), cv=10)\n",
    "\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [80, 150],\n",
    "    'min_samples_leaf': [3, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier(max_features=2)\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "param_grid.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from statistics import mean\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, shuffle=True)\n",
    "model=grid_search.best_estimator_ \n",
    "\n",
    "results = model_selection.cross_validate(estimator=model,\n",
    "                                          X=x_train,\n",
    "                                          y=y_train.values.ravel(),\n",
    "                                          cv=kfold,\n",
    "                                          scoring=scoring)\n",
    "\n",
    "print(\"F1-score = \",mean(results['test_f1_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(max_features='auto')\n",
    "# rf.fit(x_train, y_train.values.ravel())\n",
    "# print(rf.score(x_train, y_train))\n",
    "print(cross_val_score(grid_search.best_estimator_, x_train, y_train.values.ravel(), cv=10).mean()*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new = list()\n",
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i] == 1):\n",
    "        y_pred_new.append(matches.iloc[i+756].team1)\n",
    "    else:\n",
    "        y_pred_new.append(matches.iloc[i+756].team2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_check = matches.copy()\n",
    "winner_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "winner_check = winner_check.drop(columns=['season', 'city', 'toss_winner', 'toss_decision', 'win_by_runs', 'win_by_wickets', 'venue', 'umpire1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_check_new = winner_check[756:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_check_new['predWinner'] = y_pred_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_check_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
